{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ad1291-fbab-4384-9c9d-a1a683b8aa9d",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-11T07:19:04.728781Z",
     "shell.execute_reply.started": "2023-04-11T07:19:04.716243Z",
     "to_execute": "2023-04-11T07:19:04.640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Top 400 images from coco validation set by ranking the top-3 Clip scopre with the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bc4548-c8cc-4f91-9723-a9ae745a8b6e",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-11T07:19:31.428044Z",
     "shell.execute_reply.started": "2023-04-11T07:19:04.891304Z",
     "to_execute": "2023-04-11T07:19:04.815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
      "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "path_to_coco = \"./coco\"\n",
    "# coco captions\n",
    "with open(path_to_coco + \"/annotations/captions_val2017.json\", \"r\") as f:\n",
    "    caption = json.load(f)\n",
    "    print(caption.keys())\n",
    "\n",
    "print(len(caption['images']))\n",
    "\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "metric = CLIPScore(model_name_or_path=path_co_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4687d5e-5845-42c3-9a29-c231b2a0081b",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-11T12:15:56.758373Z",
     "shell.execute_reply.started": "2023-04-11T07:19:31.430134Z",
     "to_execute": "2023-04-11T07:19:04.941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "image_captions = []\n",
    "image_caption_scores = []\n",
    "\n",
    "for i in range(len(caption['images'])):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    select_image = i\n",
    "    captions = []\n",
    "    scores = []\n",
    "    image_path = path_to_coco + \"/images/val2017/\" + caption['images'][select_image]['file_name']\n",
    "    init_image = Image.open(image_path).convert('RGB')\n",
    "    # map image with its caption \n",
    "    for j in range(len(caption['annotations'])):\n",
    "        if caption['annotations'][j]['image_id'] == caption['images'][select_image]['id']:\n",
    "            captions.append(caption['annotations'][j]['caption'])\n",
    "            # compute clip score between the original image with its caption\n",
    "            score_clip = metric(transform(init_image), caption['annotations'][j]['caption'])\n",
    "            scores.append(score_clip.detach().numpy())\n",
    "    image_paths.append(image_path)\n",
    "    scores = np.array(scores)\n",
    "    scores_idx = np.argsort(scores)\n",
    "    scores_select = []\n",
    "    captions_select = []\n",
    "    # only consider the top-3 captions\n",
    "    for idx in scores_idx[-3:]:\n",
    "        scores_select.append(scores[idx])\n",
    "        captions_select.append(captions[idx])\n",
    "    image_caption_scores.append(np.mean(np.array(scores_select)))\n",
    "    image_captions.append(captions_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772b9b74-6051-4bdd-9adc-2751e39a4e49",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-11T12:15:56.830728Z",
     "shell.execute_reply.started": "2023-04-11T12:15:56.760770Z",
     "to_execute": "2023-04-11T07:19:05.057Z"
    }
   },
   "outputs": [],
   "source": [
    "# select top 400 images\n",
    "image_paths_final = []\n",
    "image_captions_final = []\n",
    "\n",
    "score_rank = np.argsort(image_caption_scores)\n",
    "for idx in score_rank[-400:]:\n",
    "    image_paths_final.append(image_paths[idx])\n",
    "    image_captions_final.append(image_captions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2947e93-69a8-4e49-a535-bfc654838308",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-11T12:18:20.718522Z",
     "shell.execute_reply.started": "2023-04-11T12:15:56.832472Z",
     "to_execute": "2023-04-11T07:19:05.216Z"
    }
   },
   "outputs": [],
   "source": [
    "#store selected images and captions\n",
    "import json\n",
    "caption_dict = {}\n",
    "image_folder = \"images_all/\"\n",
    "for i in range(len(image_paths_final)):\n",
    "    init_image = Image.open(image_paths_final[i]).convert('RGB')\n",
    "    image_name = image_paths_final[i].split('/')[-1]\n",
    "    save_path = image_folder + image_name\n",
    "    caption_dict[image_name] = image_captions_final[i]\n",
    "    init_image.save(save_path)\n",
    "with open(\"captions.json\", \"w\") as f:\n",
    "    json.dump(caption_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbe8a0-89fc-4b4e-aeab-a89d39094670",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "",
     "shell.execute_reply.started": "",
     "to_execute": "2023-04-12T02:55:33.185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e572387-bba9-452c-ac40-5e539e5ef036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
