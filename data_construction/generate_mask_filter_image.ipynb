{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7451cfff-666a-4602-9069-3051f4f0a429",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-20T11:33:10.035360Z",
     "shell.execute_reply.started": "2023-04-20T11:32:56.867884Z",
     "to_execute": "2023-04-20T11:32:56.813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "multiple = [\"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"multiple\", \"group\", \"various\", \"several\"] \n",
    "def judge_one_object(prompts):\n",
    "    one_object = True\n",
    "    for prompt in prompts:\n",
    "        p = prompt.lower()\n",
    "        for m in multiple:\n",
    "            if m in p:\n",
    "                one_object = False\n",
    "        if one_object == False:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# If there are multiple classes, select the main body according to the clip score between the class and image\n",
    "def find_main_class(classes, image):\n",
    "    print(classes)\n",
    "    class_candidate = []\n",
    "    for c in classes:\n",
    "        if c not in class_candidate:\n",
    "            class_candidate.append(c)\n",
    "    if len(class_candidate) == 1:\n",
    "        class_select = class_candidate[0]\n",
    "    else:\n",
    "        class_score = []\n",
    "        for c in classes:\n",
    "            cls_score = metric(transform(image), c)\n",
    "            class_score.append(cls_score.detach().numpy())\n",
    "        class_score = np.array(class_score)\n",
    "        class_score_idx = np.argsort(class_score)\n",
    "        class_select = classes[class_score_idx[-1]]\n",
    "    return class_select\n",
    "\n",
    "def crop_image(mask, image, segments):\n",
    "    L = image.size[0]\n",
    "    T = image.size[1]\n",
    "    R = 0\n",
    "    B = 0\n",
    "    for i in range(len(segments)):\n",
    "        bbox = segments[i]['bbox']\n",
    "        L = min(L, int(bbox[0]))\n",
    "        T = min(T, int(bbox[1]))\n",
    "        R = max(R, int(bbox[0]+bbox[2]))\n",
    "        B = max(B, int(bbox[1]+bbox[3]))\n",
    "    crop_h = int(max(B-T,R-L) * 1.5)\n",
    "    crop_w = crop_h\n",
    "    center_h = int((T+B)/2)\n",
    "    center_w = int((L+R)/2)\n",
    "    \n",
    "    start_h = max(0,center_h-crop_h//2)\n",
    "    start_w = max(0,center_w-crop_w//2)\n",
    "    \n",
    "    end_h = min(image.size[1],center_h+crop_h//2)\n",
    "    end_w = min(image.size[0],center_w+crop_w//2)\n",
    "    \n",
    "    image_crop = Image.fromarray(np.array(image)[start_h:end_h, start_w:end_w])\n",
    "    mask_crop = Image.fromarray(np.array(mask)[start_h:end_h, start_w:end_w])\n",
    "\n",
    "    return mask_crop, image_crop\n",
    "\n",
    "\n",
    "def generate_mask_image(segments_select, main_class, classes, init_image):\n",
    "    if judge_one_object(prompts_dict[image_names[img_idx]]):\n",
    "        for i in range(len(segments_select)):\n",
    "            if classes[-i-1] == main_class:\n",
    "                mask_gt = annToMask(segments_select[-i-1], init_image.size[0], init_image.size[1])\n",
    "                mask_out = Image.fromarray(np.uint8(mask_gt*255)).convert('RGB')\n",
    "                mask_out_crop, init_image_crop = crop_image(mask_out, init_image, [segments_select[-i-1]])\n",
    "                return mask_out_crop, init_image_crop\n",
    "        \n",
    "    else:\n",
    "        mask_gt_all = []\n",
    "        segments = []\n",
    "        for i in range(len(segments_select)):\n",
    "            if classes[-i-1] == main_class:\n",
    "                segments.append(segments_select[-i-1])\n",
    "                mask_gt = annToMask(segments_select[-i-1], init_image.size[0], init_image.size[1])\n",
    "                mask_gt_all.append(mask_gt)\n",
    "        mask_gt_all = np.array(mask_gt_all)\n",
    "        mask_gt = np.sum(mask_gt_all, axis=0, keepdims=False)\n",
    "        mask_gt[mask_gt>1.0] = 1.0\n",
    "        mask_out = Image.fromarray(np.uint8(mask_gt*255)).convert('RGB')\n",
    "        mask_gout_crop, init_image_crop = crop_image(mask_out, init_image, segments)\n",
    "        return mask_gout_crop, init_image_crop\n",
    "\n",
    "import mask as maskUtils\n",
    "def annToRLE(ann, w, h):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if type(segm) == list:\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, h, w)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif type(segm['counts']) == list:\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, h, w)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "def annToMask(ann, w, h):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = annToRLE(ann, w, h)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m      \n",
    "\n",
    "\n",
    "\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "path_co_clip = \"../../data/assets\"\n",
    "#path_co_clip = \"./clip\"\n",
    "metric = CLIPScore(model_name_or_path=path_co_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aadb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf18ad92-fe36-432a-a252-38cd2e2fda9c",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-20T11:38:20.749556Z",
     "shell.execute_reply.started": "2023-04-20T11:38:20.497203Z",
     "to_execute": "2023-04-20T11:38:20.369Z"
    }
   },
   "outputs": [],
   "source": [
    "# obtain segments for an image\n",
    "def obtain_segments(image_names, img_idx, instance):\n",
    "    segments_all = []\n",
    "    for i in range(len(instance['images'])):\n",
    "        if instance['images'][i]['file_name'] == image_names[img_idx]:\n",
    "            image_id = instance['images'][i]['id']\n",
    "    for i in range(len(instance['annotations'])):\n",
    "        if instance['annotations'][i]['image_id'] == image_id:\n",
    "            segments_all.append(instance['annotations'][i])\n",
    "\n",
    "    # obtain top-5 segments\n",
    "\n",
    "    area = []\n",
    "    for seg in segments_all:\n",
    "        area.append(seg['area'])\n",
    "    area = np.array(area)\n",
    "    area_idx = np.argsort(area)\n",
    "    segments = []\n",
    "    if len(segments_all) > 5:\n",
    "        for idx in area_idx[-5:]:\n",
    "            segments.append(segments_all[idx])\n",
    "    else:\n",
    "        for idx in area_idx:\n",
    "            segments.append(segments_all[idx])\n",
    "\n",
    "    segments_select = []\n",
    "    # filter out object less than 1/10 area of the total image if image has more than 2 objects\n",
    "    if len(segments) > 2:\n",
    "        for seg in segments:\n",
    "            if (seg['bbox'][2]*seg['bbox'][3]) > 0.05 * (instance['images'][img_idx]['height'] * instance['images'][img_idx]['width']):\n",
    "                segments_select.append(seg)\n",
    "        if len(segments_select) == 0:\n",
    "            segments_select.append(seg)\n",
    "    else:\n",
    "        segments_select = segments\n",
    "    classes = []\n",
    "    for seg in segments_select:\n",
    "        cls = seg['category_id']\n",
    "        for i in range(len(instance['categories'])):\n",
    "            if instance['categories'][i]['id'] == cls:\n",
    "                classes.append(instance['categories'][i]['name'])\n",
    "    return segments_select, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bfd75d4-fe3a-4e35-821d-1a0dc500de9e",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2023-04-20T12:06:38.193870Z",
     "shell.execute_reply.started": "2023-04-20T12:06:06.538371Z",
     "to_execute": "2023-04-20T12:06:06.468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "380\n",
      "2\n",
      "['bench', 'person']\n",
      "381\n",
      "1\n",
      "['teddy bear']\n",
      "382\n",
      "4\n",
      "['mouse', 'bottle', 'keyboard', 'tv']\n",
      "383\n",
      "2\n",
      "['keyboard', 'teddy bear']\n",
      "384\n",
      "8\n",
      "['person']\n",
      "385\n",
      "6\n",
      "['fire hydrant']\n",
      "386\n",
      "2\n",
      "['teddy bear', 'microwave']\n",
      "387\n",
      "1\n",
      "['scissors']\n",
      "388\n",
      "6\n",
      "['truck', 'car', 'truck', 'fire hydrant']\n",
      "390\n",
      "5\n",
      "['potted plant', 'dog', 'car', 'car']\n",
      "391\n",
      "3\n",
      "['remote', 'remote', 'person']\n",
      "392\n",
      "2\n",
      "['dining table', 'cake']\n",
      "393\n",
      "5\n",
      "['bird', 'potted plant', 'clock', 'apple']\n",
      "394\n",
      "2\n",
      "['dog', 'fire hydrant']\n",
      "395\n",
      "3\n",
      "['knife', 'cup', 'dining table']\n",
      "396\n",
      "1\n",
      "['truck']\n",
      "397\n",
      "9\n",
      "['fire hydrant']\n",
      "398\n",
      "15\n",
      "['motorcycle']\n",
      "399\n",
      "28\n",
      "['person']\n"
     ]
    }
   ],
   "source": [
    "with open(\"prompts.json\", \"r\") as f:\n",
    "    prompts_dict = json.load(f)\n",
    "image_names = list(prompts_dict.keys())\n",
    "\n",
    "with open(path_to_coco + \"/annotations/instances_val2017.json\", \"r\") as f:\n",
    "    instance = json.load(f)\n",
    "\n",
    "image_folder = \"images_crop/\"\n",
    "mask_folder = \"mask_crop/\"\n",
    "for img_idx in range(len(image_names)):\n",
    "    # skip some error images\n",
    "    if img_idx in [64, 141, 181, 219, 344, 359, 379, 389]:\n",
    "        continue\n",
    "    init_image = Image.open(path_to_coco + \"/images/val2017/\" + image_names[img_idx]).convert('RGB')\n",
    "    save_path = image_folder + image_names[img_idx]\n",
    "    mask_path = mask_folder + image_names[img_idx]\n",
    "    segments_select, classes = obtain_segments(image_names, img_idx, instance)\n",
    "    main_class = find_main_class(classes, init_image)\n",
    "    mask_crop, init_image_crop = generate_mask_image(segments_select, main_class, classes, init_image)\n",
    "    \n",
    "    init_image_crop.save(save_path)\n",
    "    mask_crop.save(mask_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
