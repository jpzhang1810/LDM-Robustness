{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e9884a-eb74-44bd-a04a-8967e9429b78",
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1016e667d03b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from diffusersgrad import StableDiffusionInpaintPipeline\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from utils import preprocess, recover_image\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "model_id_or_path = \"runwayml/stable-diffusion-inpainting\"\n",
    "#model_id_or_path = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "\n",
    "image_inpainting = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_id_or_path,\n",
    "    revision=\"fp16\", \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "image_inpainting = image_inpainting.to(\"cuda\")\n",
    "\n",
    "with open(\"dataset/prompts.json\", \"r\") as f:\n",
    "    prompts_dict = json.load(f)\n",
    "image_names = list(prompts_dict.keys())\n",
    "\n",
    "# Selected image ids for image inpainting model\n",
    "image_idxs = [0,2,7,8,16,17,19,20,23,25,26,27,31,33,35,36,41,42,43,44,45,46,49,50,51,52,54,55,56,58,61,63,67,71,\n",
    "77,78,82,83,85,87,88,89,91,92,97,98,99,100,116,118,129,131,133,134,137,138,148,155,157,159,162,166,167,172,\n",
    "178,182,183,184,186,189,192,193,195,197,199,201,206,207,208,209,212,213,214,215,216,217,231,235,237,239,240,\n",
    "241,250,251,252,254,255,256,266,268,269,271,272,279,285,286,287,288,293,299,300,301,310,321,326,327,331,332,\n",
    "333,338,339,343,345,349,351,355,356,361,363,364,367,369,370,375,377,378,380,382,383,387,390,391,397]\n",
    "\n",
    "# A fixed random selected seed in all the experiments\n",
    "SEED = 9209\n",
    "torch.manual_seed(SEED)\n",
    "strength = 0.7\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 100\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "MSE = torch.nn.MSELoss()\n",
    "\n",
    "class BIM_inpainting(object):\n",
    "    def __init__(self, model, epsilon=0.1, iteration=15, step_length=0.01):\n",
    "        self.model = model\n",
    "        self.record_features = []\n",
    "        self._register_model()\n",
    "        self.eps = epsilon\n",
    "        self.T = iteration\n",
    "        self.step_length = step_length\n",
    "        self.feature_ori = []\n",
    "        \n",
    "    def _register_model(self): \n",
    "        def obtain_output_feature(module, feature_in, feature_out):\n",
    "            self.record_features.append(feature_out[0])\n",
    "        # Encoding\n",
    "        self.hook = self.model.vae.encoder.register_forward_hook(obtain_output_feature) # encoder\n",
    "        #self.hook = self.model.vae.quant_conv.register_forward_hook(obtain_output_feature) # quant conv\n",
    "        \n",
    "        # Unet\n",
    "        #self.hook = self.model.unet.down_blocks[1].attentions[0].transformer_blocks[0].attn1.register_forward_hook(obtain_output_feature) # self-attn\n",
    "        #self.hook = self.model.unet.down_blocks[1].attentions[0].transformer_blocks[0].attn2.register_forward_hook(obtain_output_feature) # cross-attn\n",
    "        #self.hook = self.model.unet.down_blocks[1].attentions[0].transformer_blocks[0].ff.register_forward_hook(obtain_output_feature) # feed-forward\n",
    "        #self.hook = self.model.unet.down_blocks[1].resnets[0].register_forward_hook(obtain_output_feature) # resnet\n",
    "        \n",
    "        # Decoding\n",
    "        #self.hook = self.model.vae.post_quant_conv.register_forward_hook(obtain_output_feature) # post quant conv\n",
    "        #self.hook = self.model.vae.decoder.register_forward_hook(obtain_output_feature) # decoder\n",
    "        \n",
    "        # other trials\n",
    "        # downblock is the best choice\n",
    "        #self.hook = self.model.unet.mid_block.attentions[0].transformer_blocks[0].attn1.register_forward_hook(obtain_output_feature)\n",
    "        #self.hook = self.model.unet.up_blocks[2].attentions[0].transformer_blocks[0].attn2.register_forward_hook(obtain_output_feature)\n",
    "\n",
    "    def generate(self, image, prompt, mask_image, strength=0.7,guidance_scale=7.5,num_inference_steps=100):\n",
    "        with torch.no_grad():\n",
    "            torch.manual_seed(SEED)\n",
    "            #img_tmp = self.model(prompt=prompt, image=image, mask_image = mask_image, strength=strength, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "            img_tmp = self.model(prompt=prompt, image=image, mask_image = mask_image, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "        return img_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332332e-2ee0-412a-b033-85f7affda78a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "folder = \"data/inpainting/encoder/\"\n",
    "folder_save = \"data/inpainting/encoder_generate/\"\n",
    "\n",
    "mask_folder = \"dataset/mask_crop/\"\n",
    "\n",
    "if not os.path.exists(folder_save):\n",
    "    os.mkdir(folder_save)\n",
    "    \n",
    "attack = BIM_inpainting(image_inpainting)\n",
    "\n",
    "# We do experiments on 100*5 data triplets\n",
    "total = 0\n",
    "for image_idx in image_idxs:\n",
    "    torch.cuda.empty_cache()\n",
    "    prompts = prompts_dict[image_names[image_idx]]\n",
    "    mask_image = Image.open(mask_folder + image_names[image_idx]).convert('RGB').resize((512,512))\n",
    "    mask_image = ImageOps.invert(mask_image).resize((512,512))\n",
    "    for j in range(5):\n",
    "        ori_image = Image.open(folder + image_names[image_idx][:-4]+\"_\"+str(j)+\".png\").convert('RGB').resize((512,512))\n",
    "        prompt = prompts[j]\n",
    "        torch.manual_seed(SEED)\n",
    "        img = attack.generate(ori_image, prompt, mask_image)\n",
    "        save_path = folder_save + image_names[image_idx][:-4]+ \"_\" + str(j) + image_names[image_idx][-4:]\n",
    "        img.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15020f9a-71e9-494d-913a-e149336ef9ae",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
