{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a4cf8-c818-4444-a55f-b3e172ebd95e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from diffusersgrad import StableDiffusionImg2ImgPipeline\n",
    "import torchvision.transforms as T\n",
    "from torchmetrics.multimodal import *\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "from torchmetrics.image import MultiScaleStructuralSimilarityIndexMeasure\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from utils import preprocess, recover_image\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "with open(\"dataset/prompts.json\", \"r\") as f:\n",
    "    prompts_dict = json.load(f)\n",
    "image_names = list(prompts_dict.keys())\n",
    "\n",
    "image_idxs = [0,2,7,8,16,17,19,20,23,25,26,27,31,33,35,36,41,42,43,44,45,46,49,50,51,52,54,55,56,58,61,63,67,71,\n",
    "77,78,82,83,85,87,88,89,91,92,97,98,99,100,116,118,129,131,133,134,137,138,148,155,157,159,162,166,167,172,\n",
    "178,182,183,184,186,189,192,193,195,197,199,201,206,207,208,209,212,213,214,215,216,217,231,235,237,239,240,\n",
    "241,250,251,252,254,255,256,266,268,269,271,272,279,285,286,287,288,293,299,300,301,310,321,326,327,331,332,\n",
    "333,338,339,343,345,349,351,355,356,361,363,364,367,369,370,375,377,378,380,382,383,387,390,391,397]\n",
    "\n",
    "\n",
    "# A fixed random selected seed in all the experiments\n",
    "SEED = 9209\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "fid = FrechetInceptionDistance()\n",
    "inception = InceptionScore()\n",
    "psnr = PeakSignalNoiseRatio(data_range=1.0)\n",
    "ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6d265-9de4-4cfb-a3b1-24bcb57bc04b",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Generated images by adversarial samples\n",
    "folder = \"data/inpainting/encoder_generate/\"\n",
    "# Generated images by original samples\n",
    "folder_ori_generate = \"data/inpainting/ori_images_generate/\"\n",
    "# Original images\n",
    "folder_ori = \"dataset/images_crop/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689c3d2-359f-4452-bba6-703c7eed751a",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#FID\n",
    "total = 0\n",
    "scores = 0.0\n",
    "num = 100\n",
    "image_advs = []\n",
    "image_oris = []\n",
    "for i in range(num):\n",
    "    image_idx = image_idxs[i]\n",
    "    torch.cuda.empty_cache()\n",
    "    for j in range(5):\n",
    "        adv = Image.open(folder + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        ori = Image.open(folder_ori + image_names[image_idx]).convert('RGB').resize((512,512))\n",
    "        torch.manual_seed(SEED)\n",
    "        adv = np.array(adv).astype(np.uint8)\n",
    "        adv = adv[None].transpose(0, 3, 1, 2)\n",
    "        image_adv = torch.from_numpy(adv)\n",
    "        image_advs.append(image_adv)\n",
    "        ori = np.array(ori).astype(np.uint8)\n",
    "        ori = ori[None].transpose(0, 3, 1, 2)\n",
    "        image_ori = torch.from_numpy(ori)\n",
    "        image_oris.append(image_ori)\n",
    "\n",
    "image_advs = torch.concat(image_advs, dim=0)\n",
    "image_oris = torch.concat(image_oris, dim=0)\n",
    "print(image_advs.shape)\n",
    "fid.update(image_advs, real=False)\n",
    "fid.update(image_oris, real=True)\n",
    "torch.manual_seed(SEED)\n",
    "score = fid.compute()\n",
    "score = score.detach().cpu().numpy()\n",
    "print(\"FID: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116912e-d561-46ce-926c-bd5557c3fa57",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#IS\n",
    "total = 0\n",
    "scores = 0.0\n",
    "num = 100\n",
    "image_advs = []\n",
    "image_oris = []\n",
    "for i in range(num):\n",
    "    image_idx = image_idxs[i]\n",
    "    torch.cuda.empty_cache()\n",
    "    for j in range(5):\n",
    "        adv = Image.open(folder + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        ori = Image.open(folder_ori + image_names[image_idx]).convert('RGB').resize((512,512))\n",
    "        torch.manual_seed(SEED)\n",
    "        adv = np.array(adv).astype(np.uint8)\n",
    "        adv = adv[None].transpose(0, 3, 1, 2)\n",
    "        image_adv = torch.from_numpy(adv)\n",
    "        image_advs.append(image_adv)\n",
    "        ori = np.array(ori).astype(np.uint8)\n",
    "        ori = ori[None].transpose(0, 3, 1, 2)\n",
    "        image_ori = torch.from_numpy(ori)\n",
    "        image_oris.append(image_ori)\n",
    "\n",
    "image_advs = torch.concat(image_advs, dim=0)\n",
    "image_oris = torch.concat(image_oris, dim=0)\n",
    "print(image_advs.shape)\n",
    "inception.update(image_advs)\n",
    "torch.manual_seed(SEED)\n",
    "score = inception.compute()\n",
    "IS = score[0].detach().cpu().numpy()\n",
    "print(\"IS: \", IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb928ae-70be-4b4a-8dfd-1e7364edf346",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#PSNR\n",
    "total = 0\n",
    "scores = 0.0\n",
    "num = 100\n",
    "image_advs = []\n",
    "image_oris = []\n",
    "for i in range(num):\n",
    "    image_idx = image_idxs[i]\n",
    "    torch.cuda.empty_cache()\n",
    "    for j in range(5):\n",
    "        adv = Image.open(folder + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        ori = Image.open(folder_ori_generate + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        torch.manual_seed(SEED)\n",
    "        adv = np.array(adv).astype(np.float32)/255.0\n",
    "        adv = adv[None].transpose(0, 3, 1, 2)\n",
    "        image_adv = torch.from_numpy(adv)\n",
    "        image_advs.append(image_adv)\n",
    "        ori = np.array(ori).astype(np.float32)/255.0\n",
    "        ori = ori[None].transpose(0, 3, 1, 2)\n",
    "        image_ori = torch.from_numpy(ori)\n",
    "        image_oris.append(image_ori)\n",
    "\n",
    "image_advs = torch.concat(image_advs, dim=0)\n",
    "image_oris = torch.concat(image_oris, dim=0)\n",
    "print(image_advs.shape)\n",
    "torch.manual_seed(SEED)\n",
    "score = psnr(image_advs, image_oris)\n",
    "\n",
    "print(\"PSNR: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94b024-6607-432d-a777-cb13a6742dae",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#SSIM\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "total = 0\n",
    "scores = 0.0\n",
    "num = 100\n",
    "image_advs = []\n",
    "image_oris = []\n",
    "for i in range(num):\n",
    "    image_idx = image_idxs[i]\n",
    "    torch.cuda.empty_cache()\n",
    "    for j in range(5):\n",
    "        adv = Image.open(folder + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        ori = Image.open(folder_ori_generate + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        torch.manual_seed(SEED)\n",
    "        adv = np.array(adv).astype(np.float32)\n",
    "        adv = adv[None].transpose(0, 3, 1, 2)\n",
    "        image_adv = torch.from_numpy(adv)\n",
    "        image_advs.append(image_adv)\n",
    "        ori = np.array(ori).astype(np.float32)\n",
    "        ori = ori[None].transpose(0, 3, 1, 2)\n",
    "        image_ori = torch.from_numpy(ori)\n",
    "        image_oris.append(image_ori)\n",
    "\n",
    "image_advs = torch.concat(image_advs, dim=0)\n",
    "image_oris = torch.concat(image_oris, dim=0)\n",
    "print(image_advs.shape)\n",
    "torch.manual_seed(SEED)\n",
    "score = ssim(image_advs, image_oris)\n",
    "\n",
    "print(\"SSIM: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0d682-ae12-4b8d-b2d2-1c9d8f944a88",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#MSSSIM\n",
    "msssim = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "total = 0\n",
    "scores = 0.0\n",
    "num = 100\n",
    "image_advs = []\n",
    "image_oris = []\n",
    "for i in range(num):\n",
    "    image_idx = image_idxs[i]\n",
    "    torch.cuda.empty_cache()\n",
    "    for j in range(5):\n",
    "        adv = Image.open(folder + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        ori = Image.open(folder_ori_generate + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        torch.manual_seed(SEED)\n",
    "        adv = np.array(adv).astype(np.float32)\n",
    "        adv = adv[None].transpose(0, 3, 1, 2)\n",
    "        image_adv = torch.from_numpy(adv)\n",
    "        image_advs.append(image_adv)\n",
    "        ori = np.array(ori).astype(np.float32)\n",
    "        ori = ori[None].transpose(0, 3, 1, 2)\n",
    "        image_ori = torch.from_numpy(ori)\n",
    "        image_oris.append(image_ori)\n",
    "\n",
    "image_advs = torch.concat(image_advs, dim=0)\n",
    "image_oris = torch.concat(image_oris, dim=0)\n",
    "print(image_advs.shape)\n",
    "torch.manual_seed(SEED)\n",
    "score = msssim(image_advs, image_oris)\n",
    "\n",
    "print(\"MSSSIM: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d50cb4-374d-4c1b-9e60-87cc4096540d",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#CLIP Score\n",
    "total = 0\n",
    "scores = 0.0\n",
    "num = 100\n",
    "for i in range(num):\n",
    "    image_idx = image_idxs[i]\n",
    "    torch.cuda.empty_cache()\n",
    "    if total % 20 == 0:\n",
    "        print(total)\n",
    "    prompts = prompts_dict[image_names[image_idx]]\n",
    "    for j in range(5):\n",
    "        image = Image.open(folder + image_names[image_idx][:-4] + \"_\" + str(j) + image_names[image_idx][-4:]).convert('RGB').resize((512,512))\n",
    "        prompt = prompts[j]\n",
    "        torch.manual_seed(SEED)\n",
    "        score = metric(transform(image), prompt)\n",
    "        score = score.detach().cpu().numpy()\n",
    "        scores += score\n",
    "    total += 1\n",
    "print(scores/(num*5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
