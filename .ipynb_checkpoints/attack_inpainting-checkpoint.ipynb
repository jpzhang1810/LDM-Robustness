{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da8b7c-860c-4b04-bc49-b7bd28dec665",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "\n",
    "from diffusersgrad import StableDiffusionInpaintPipeline\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from utils import preprocess, recover_image\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "model_id_or_path = \"runwayml/stable-diffusion-inpainting\"\n",
    "#model_id_or_path = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "\n",
    "image_inpainting = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_id_or_path,\n",
    "    revision=\"fp16\", \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "image_inpainting = image_inpainting.to(\"cuda\")\n",
    "\n",
    "with open(\"dataset/prompts.json\", \"r\") as f:\n",
    "    prompts_dict = json.load(f)\n",
    "image_names = list(prompts_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffe1de-0773-4cbe-91ea-6538faec4681",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Selected image ids for image inpainting model\n",
    "image_idxs = [0,2,7,8,16,17,19,20,23,25,26,27,31,33,35,36,41,42,43,44,45,46,49,50,51,52,54,55,56,58,61,63,67,71,\n",
    "77,78,82,83,85,87,88,89,91,92,97,98,99,100,116,118,129,131,133,134,137,138,148,155,157,159,162,166,167,172,\n",
    "178,182,183,184,186,189,192,193,195,197,199,201,206,207,208,209,212,213,214,215,216,217,231,235,237,239,240,\n",
    "241,250,251,252,254,255,256,266,268,269,271,272,279,285,286,287,288,293,299,300,301,310,321,326,327,331,332,\n",
    "333,338,339,343,345,349,351,355,356,361,363,364,367,369,370,375,377,378,380,382,383,387,390,391,397]\n",
    "\n",
    "# A fixed random selected seed in all the experiments\n",
    "SEED = 9209\n",
    "torch.manual_seed(SEED)\n",
    "strength = 0.7\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 100\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "MSE = torch.nn.MSELoss()\n",
    "\n",
    "class BIM_inpainting(object):\n",
    "    def __init__(self, model, epsilon=0.1, iteration=15, step_length=0.01):\n",
    "        self.model = model\n",
    "        self.record_features = []\n",
    "        self._register_model()\n",
    "        self.eps = epsilon\n",
    "        self.T = iteration\n",
    "        self.step_length = step_length\n",
    "        self.feature_ori = []\n",
    "        \n",
    "    def _register_model(self): \n",
    "        def obtain_output_feature(module, feature_in, feature_out):\n",
    "            self.record_features.append(feature_out[0])\n",
    "        # Encoding\n",
    "        self.hook = self.model.vae.encoder.register_forward_hook(obtain_output_feature) # encoder\n",
    "        #self.hook = self.model.vae.quant_conv.register_forward_hook(obtain_output_feature) # quant conv\n",
    "        \n",
    "        # Unet\n",
    "        #self.hook = self.model.unet.down_blocks[1].attentions[0].transformer_blocks[0].attn1.register_forward_hook(obtain_output_feature) # self-attn\n",
    "        #self.hook = self.model.unet.down_blocks[1].attentions[0].transformer_blocks[0].attn2.register_forward_hook(obtain_output_feature) # cross-attn\n",
    "        #self.hook = self.model.unet.down_blocks[1].attentions[0].transformer_blocks[0].ff.register_forward_hook(obtain_output_feature) # feed-forward\n",
    "        #self.hook = self.model.unet.down_blocks[1].resnets[0].register_forward_hook(obtain_output_feature) # resnet\n",
    "        \n",
    "        # Decoding\n",
    "        #self.hook = self.model.vae.post_quant_conv.register_forward_hook(obtain_output_feature) # post quant conv\n",
    "        #self.hook = self.model.vae.decoder.register_forward_hook(obtain_output_feature) # decoder\n",
    "        \n",
    "        # other trials\n",
    "        # downblock is the best choice\n",
    "        #self.hook = self.model.unet.mid_block.attentions[0].transformer_blocks[0].attn1.register_forward_hook(obtain_output_feature)\n",
    "        #self.hook = self.model.unet.up_blocks[2].attentions[0].transformer_blocks[0].attn2.register_forward_hook(obtain_output_feature)\n",
    "\n",
    "    def generate(self, image, prompt, mask_image, strength=0.7,guidance_scale=7.5,num_inference_steps=100):\n",
    "        with torch.no_grad():\n",
    "            torch.manual_seed(SEED)\n",
    "            img_tmp = self.model(prompt=prompt, image=image, mask_image = mask_image, strength=strength, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "        return img_tmp\n",
    "    \n",
    "    def attack(self, ori_image, prompt, mask_image, strength=0.7,guidance_scale=7.5,num_inference_steps=15):\n",
    "        for i in range(self.T):\n",
    "            if i==0:\n",
    "                self.record_features = []\n",
    "                torch.manual_seed(SEED)\n",
    "                with torch.no_grad():\n",
    "                    #img_tmp = self.model(prompt=prompt, image=ori_image, mask_image = mask_image, strength=strength, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "                    img_tmp = self.model(prompt=prompt, image=ori_image, mask_image = mask_image, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "                for f_id in range(len(self.record_features)):\n",
    "                    self.feature_ori.append(self.record_features[f_id])\n",
    "                    \n",
    "                ori = np.array(ori_image).astype(np.float32) / 255.0\n",
    "                ori = ori[None].transpose(0, 3, 1, 2)\n",
    "                ori_image = torch.from_numpy(ori)\n",
    "                ori_mask = np.array(mask_image).astype(np.float32) / 255.0\n",
    "                ori_mask = ori_mask[None].transpose(0, 3, 1, 2)\n",
    "                ori_mask_image = torch.from_numpy(ori_mask)\n",
    "                ori_mask_image = ori_mask_image[:,0,:,:]\n",
    "                # initialize with a small noise to start attack\n",
    "                adv_image = ori_image+torch.normal(0.0, 0.1, size=ori_image.shape)\n",
    "                adv_image = torch.clamp(adv_image,0.0,1.0)\n",
    "                adv_image = adv_image.cuda()\n",
    "                ori_image = ori_image.cuda()\n",
    "                ori_mask_image = ori_mask_image.cuda()\n",
    "                adv_image.requires_grad_()\n",
    "            del self.record_features\n",
    "            self.record_features = []\n",
    "            torch.manual_seed(SEED)\n",
    "            adv_image.requires_grad_()\n",
    "            torch.cuda.empty_cache()\n",
    "            #img_tmp = self.model(prompt=prompt, image=adv_image, mask_image = ori_mask_image, strength=strength, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "            img_tmp = self.model(prompt=prompt, image=adv_image, mask_image = ori_mask_image, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "            \n",
    "            cost = torch.tensor(0).half().cuda()\n",
    "            for f_id in range(len(self.record_features)):\n",
    "                cost += MSE(self.record_features[f_id], self.feature_ori[f_id])\n",
    "            cost = cost.requires_grad_()\n",
    "            grad, = torch.autograd.grad(cost, [adv_image])\n",
    "\n",
    "            grad = grad/torch.mean(torch.abs(grad), dim=[1,2,3], keepdim=True)\n",
    "            adv_image = adv_image + self.step_length * grad.sign()\n",
    "            pert = torch.clamp(adv_image - ori_image, -self.eps, self.eps)\n",
    "            adv_image = ori_image + pert\n",
    "            adv_image = torch.clamp(adv_image,0.0,1.0)\n",
    "            del pert, grad, cost, img_tmp\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return adv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea0a09-8576-490b-9e17-58f9cfceb6e1",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "image_folder = \"dataset/images_crop/\"\n",
    "mask_folder = \"dataset/mask_crop/\"\n",
    "folder = \"data/inpainting/encoder/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "attack = BIM_inpainting(image_inpainting)\n",
    "\n",
    "# We do experiments on 100*5 data triplets\n",
    "start = 0\n",
    "for i in range(start, 100):\n",
    "    image_idx = image_idxs[i]\n",
    "    print(i)\n",
    "    torch.cuda.empty_cache()\n",
    "    prompts = prompts_dict[image_names[image_idx]]\n",
    "    ori_image = Image.open(image_folder + image_names[image_idx]).convert('RGB').resize((512,512))\n",
    "    mask_image = Image.open(mask_folder + image_names[image_idx]).convert('RGB').resize((512,512))\n",
    "    mask_image = ImageOps.invert(mask_image).resize((512,512))\n",
    "    for j in range(5):\n",
    "        prompt = prompts[j]\n",
    "        torch.manual_seed(SEED)\n",
    "        img = attack.attack(ori_image, prompt, mask_image)\n",
    "        img = to_pil(img[0]).convert(\"RGB\")\n",
    "        save_path = folder + image_names[image_idx][:-4]+ \"_\" + str(j) + \".png\"\n",
    "        img.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
